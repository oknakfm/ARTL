{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPeeAy00ybFjvg4DnFK+jcu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Table 2"],"metadata":{"id":"pK42E8oGnqHu"}},{"cell_type":"code","source":["import requests\n","import pandas as pd\n","import seaborn as sns\n","from io import BytesIO\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","import os\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","EPOCHS = 5000  # エポック数\n","\n","# AutoMPG dataset\n","def load_auto_mpg():\n","    auto_mpg = sns.load_dataset(\"mpg\")\n","    # remove 'car_name'\n","    auto_mpg.drop(columns=['name','origin'], inplace=True)\n","    auto_mpg.dropna(inplace=True)\n","    return auto_mpg\n","\n","# Liver Disorders dataset\n","def load_liver_disorders():\n","    url_liver = 'http://archive.ics.uci.edu/ml/machine-learning-databases/liver-disorders/bupa.data'\n","    columns_liver = ['mcv', 'alkphos', 'sgpt', 'sgot', 'gammagt', 'drinks', 'selector']\n","    liver = pd.read_csv(url_liver, names=columns_liver)\n","    # remove 'selector'\n","    liver.drop(columns=['selector'], inplace=True)\n","    return liver\n","\n","# Real Estate Evaluation dataset\n","def load_real_estate():\n","    file_path = '/content/drive/MyDrive/ARTL/Real_estate.csv'\n","    real_estate = pd.read_csv(file_path)\n","    real_estate.drop(columns=['X1 transaction date', 'X5 latitude', 'X6 longitude'], inplace=True)\n","    return real_estate\n","\n","def load_datasets():\n","    try:\n","        auto_mpg = load_auto_mpg()\n","        print(\"Auto MPG dataset loaded successfully.\")\n","\n","        liver = load_liver_disorders()\n","        print(\"Liver Disorders dataset loaded successfully.\")\n","\n","        real_estate = load_real_estate()\n","        print(\"Real Estate dataset loaded successfully.\")\n","\n","        datasets = {\n","            'Auto MPG': (auto_mpg, 'mpg'),\n","            'Liver Disorders': (liver, 'drinks'),\n","            'Real Estate': (real_estate, 'Y house price of unit area')\n","        }\n","        return datasets\n","\n","    except Exception as e:\n","        print(f\"Error loading datasets: {e}\")\n","        return None\n","\n","class NeuralNetwork(nn.Module):\n","    def __init__(self, input_dim, activation_func='sigmoid'):\n","        super(NeuralNetwork, self).__init__()\n","        self.first = nn.Linear(input_dim, 100)\n","        self.hidden1 = nn.Linear(100, 100)\n","        self.hidden2 = nn.Linear(100, 100)\n","        self.hidden3 = nn.Linear(100, 100)\n","        self.output = nn.Linear(100, 1)\n","        if activation_func == 'sigmoid':\n","            self.activation = nn.Sigmoid()\n","        else:\n","            self.activation = nn.ReLU()\n","    def forward(self, x):\n","        x = self.activation(self.first(x))\n","        x = self.activation(self.hidden1(x))\n","        x = self.activation(self.hidden2(x))\n","        x = self.activation(self.hidden3(x))\n","        return self.output(x)\n","\n","# HOVR\n","def hovr_regularization(model, x, k=2, q=2, M=10):\n","    x_min, x_max = x.min(0)[0], x.max(0)[0]\n","    random_points = torch.tensor(\n","        np.random.uniform(x_min.cpu().numpy(), x_max.cpu().numpy(), (M, x.shape[1])),\n","        dtype=torch.float32, requires_grad=True).to(device)\n","    preds = model(random_points)\n","    grads = torch.autograd.grad(preds, random_points, torch.ones_like(preds),\n","                                create_graph=True)[0]\n","    hovr_term = 0.0\n","    n_dims = x.shape[1]\n","    for i in range(n_dims):\n","        grad_i = grads[:, i]\n","        temp_grad = grad_i\n","        for _ in range(k - 1):\n","            temp_grad = torch.autograd.grad(temp_grad, random_points,\n","                                            torch.ones_like(temp_grad),\n","                                            create_graph=True)[0][:, i]\n","        hovr_term += (1 / n_dims) * torch.sum(torch.abs(temp_grad) ** q)\n","    return hovr_term\n","\n","# TTL+HOVR\n","def transformed_ttl_hovr_loss(model, xi, x, y, h, lambd, k, q):\n","    n = x.shape[0]\n","    preds = model(x)\n","    residuals = (y - preds).view(-1, 1)\n","    loss_fit = (1 / n) * torch.sum((residuals - xi) ** 2)\n","    xi_squared = xi.view(-1) ** 2\n","    T_h_xi = (1 / n) * torch.sum(torch.topk(xi_squared, h, largest=False)[0])\n","    hovr_term = lambd * hovr_regularization(model, x, k, q)\n","    total_loss = loss_fit + T_h_xi + hovr_term\n","    return total_loss\n","\n","def create_directory(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","def save_results(data_name, trial, method_name, pmse):\n","    # Google Drive\n","    create_directory('/content/drive/MyDrive/ARTL/benchmark_experiment')\n","    filename = f'/content/drive/MyDrive/ARTL/benchmark_experiment/{data_name}_{method_name}_trial_{trial}.csv'\n","    df = pd.DataFrame({'pmse': [pmse]})\n","    df.to_csv(filename, index=False)\n","\n","def evaluate_methods(data_name, X, y, y_col):\n","    pmse_results = []\n","    input_dim = X.shape[1]\n","    for trial in range(1, 6):\n","        print(f\"Dataset: {data_name}, Trial: {trial}\")\n","        # Split dataset into train/test\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=trial)\n","        # Outliers\n","        n_outliers = int(0.05 * len(y_train))\n","        outlier_indices = np.random.choice(len(y_train), n_outliers, replace=False)\n","        std_y = y_train.std()\n","        y_train.iloc[outlier_indices] += 2 * std_y\n","        # Baselines\n","        methods = [\n","            ('NN with Huber Loss', nn_huber_loss),\n","            ('NN with Tukey Loss', nn_tukey_loss),\n","            ('NN with RANSAC', nn_ransac_like),\n","            ('NN with Label Noise Reg.', label_noise_regularization),\n","        ]\n","        # TTL+HOVR parameters\n","        lambdas = [1e-3, 1e-4]\n","        ks = [1, 2]\n","        for lambd in lambdas:\n","            for k in ks:\n","                method_name = f'NN with TTL+HOVR(k={k}, λ={lambd})'\n","                methods.append((method_name, lambda X_train, y_train, X_test, y_test, trial, outlier_indices, input_dim:\n","                                ttl_hovr(X_train, y_train, X_test, y_test, trial,\n","                                         outlier_indices, k=k, lambd=lambd, input_dim=input_dim)))\n","        for method_name, method_function in methods:\n","            print(f\"Running method: {method_name}\")\n","            pmse = method_function(X_train, y_train, X_test, y_test, trial, outlier_indices, input_dim)\n","            save_results(data_name, trial, method_name, pmse)\n","            pmse_results.append({'dataset': data_name,\n","                                 'method': method_name,\n","                                 'trial': trial,\n","                                 'pmse': pmse})\n","    return pmse_results\n","\n","# Baselines\n","def nn_huber_loss(X_train, y_train, X_test, y_test, trial, outlier_indices, input_dim):\n","    model = NeuralNetwork(input_dim).to(device)\n","    criterion = nn.SmoothL1Loss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.01)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n","    X_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n","    y_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32).to(device)\n","    for epoch in range(EPOCHS):\n","        optimizer.zero_grad()\n","        outputs = model(X_tensor)\n","        loss = criterion(outputs, y_tensor)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","    with torch.no_grad():\n","        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n","        y_pred = model(X_test_tensor).cpu().numpy().flatten()\n","    pmse = mean_squared_error(y_test, y_pred)\n","    return pmse\n","\n","def nn_tukey_loss(X_train, y_train, X_test, y_test, trial, outlier_indices, input_dim):\n","    def tukey_loss(output, target, c=4.685):\n","        residual = target - output\n","        abs_residual = torch.abs(residual)\n","        mask = abs_residual <= c\n","        loss = torch.zeros_like(residual)\n","        loss[mask] = (c**2 / 6)*(1 - (1 - (residual[mask]/c)**2)**3)\n","        loss[~mask] = (c**2 / 6)\n","        return torch.mean(loss)\n","    model = NeuralNetwork(input_dim).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=0.01)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n","    X_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n","    y_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32).to(device)\n","    for epoch in range(EPOCHS):\n","        optimizer.zero_grad()\n","        outputs = model(X_tensor)\n","        loss = tukey_loss(outputs, y_tensor)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","    with torch.no_grad():\n","        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n","        y_pred = model(X_test_tensor).cpu().numpy().flatten()\n","    pmse = mean_squared_error(y_test, y_pred)\n","    return pmse\n","\n","def nn_ransac_like(X_train, y_train, X_test, y_test, trial, outlier_indices, input_dim):\n","    model = NeuralNetwork(input_dim).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=0.01)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n","    X_t = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n","    y_t = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32).to(device)\n","    for epoch in range(1000):\n","        optimizer.zero_grad()\n","        outputs = model(X_t)\n","        loss = nn.MSELoss()(outputs, y_t)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","    with torch.no_grad():\n","        residuals = torch.abs(y_t - model(X_t))\n","    threshold = torch.quantile(residuals, 0.90)\n","    inliers = residuals <= threshold\n","    X_inliers = X_t[inliers.view(-1)]\n","    y_inliers = y_t[inliers]\n","    for epoch in range(EPOCHS):\n","        optimizer.zero_grad()\n","        outputs = model(X_inliers)\n","        loss = nn.MSELoss()(outputs, y_inliers)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","    with torch.no_grad():\n","        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n","        y_pred = model(X_test_tensor).cpu().numpy().flatten()\n","    pmse = mean_squared_error(y_test, y_pred)\n","    return pmse\n","\n","def label_noise_regularization(X_train, y_train, X_test, y_test, trial, outlier_indices, input_dim):\n","    model = NeuralNetwork(input_dim).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=0.01)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n","    criterion = nn.MSELoss()\n","    X_t = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n","    y_t = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32).to(device)\n","    smoothing = 0.1\n","    for epoch in range(EPOCHS):\n","        optimizer.zero_grad()\n","        outputs = model(X_t)\n","        targets_smoothed = y_t * (1 - smoothing) + smoothing * y_t.mean()\n","        loss = criterion(outputs, targets_smoothed)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","    with torch.no_grad():\n","        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n","        y_pred = model(X_test_tensor).cpu().numpy().flatten()\n","    pmse = mean_squared_error(y_test, y_pred)\n","    return pmse\n","\n","def ttl_hovr(X_train, y_train, X_test, y_test, trial, outlier_indices, k, lambd, input_dim):\n","    model = NeuralNetwork(input_dim).to(device)\n","    n_params = X_train.shape[0]\n","    xi = torch.zeros(n_params, 1, requires_grad=True, device=device) # augmented parameter\n","    optimizer = optim.Adam(list(model.parameters()) + [xi], lr=0.01)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n","    X_t = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n","    y_t = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32).to(device)\n","    h = int(0.95 * n_params)\n","    q = 2\n","    for epoch in range(EPOCHS):\n","        optimizer.zero_grad()\n","        loss = transformed_ttl_hovr_loss(model, xi, X_t, y_t, h, lambd, k, q)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","    with torch.no_grad():\n","        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n","        y_pred = model(X_test_tensor).cpu().numpy().flatten()\n","    pmse = mean_squared_error(y_test, y_pred)\n","    return pmse\n","\n","def main():\n","    datasets = load_datasets()\n","    all_results = []\n","    for data_name, (df, y_col) in datasets.items():\n","        if df is not None:\n","            print(f\"Processing dataset: {data_name}\")\n","            X = df.drop(y_col, axis=1)\n","            y = df[y_col]\n","            X = (X - X.mean()) / X.std()\n","            y = (y - y.mean()) / y.std()\n","            results = evaluate_methods(data_name, X, y, y_col)\n","            all_results.extend(results)\n","    df_results = pd.DataFrame(all_results)\n","    summary = df_results.groupby(['dataset', 'method'])['pmse'].agg(['mean', 'std']).reset_index()\n","    print(summary)\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"E98x2qrx2Chk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JeBE0FvWGDKX"},"execution_count":null,"outputs":[]}]}